{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "\n",
    "The aim of the notebook is demo end to end pipeline for Ads prediction in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ./setup.sh # uncomment if you wish to install any new packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow, 2.1.0 on Python interpreter, sys.version_info(major=3, minor=7, micro=5, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any, Union, List, Tuple\n",
    "from functools import partial\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from math import ceil\n",
    "from collections import namedtuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import chakin\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import zipfile\n",
    "import sqlite3\n",
    "import logging\n",
    "from tempfile import TemporaryDirectory\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from EmbeddingFactory import EmbeddingFactory\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(f\"Using Tensorflow, {tf.__version__} on Python interpreter, {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed, 1588815152\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = int(time.time())\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Using random seed, {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Dataset credits:\n",
    "```\n",
    "@inproceedings{roffo2016personality,\n",
    "  title={Personality in computational advertising: A benchmark},\n",
    "  author={Roffo, Giorgio and Vinciarelli, Alessandro},\n",
    "  booktitle={4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016},\n",
    "  pages={18},\n",
    "  year={2016}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../dataset/\")\n",
    "BATCH_SIZE = 4096 # bigger the batch, faster the training but bigger the RAM needed\n",
    "TARGET_COL = \"Rating\"\n",
    "\n",
    "# data files path are relative DATA_FOLDER\n",
    "users_ads_rating_csv = DATA_FOLDER/\"users-ads-without-gcp-ratings_OHE_MLB_FAV_UNFAV_Merged.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "USER_ID = \"UserId\"\n",
    "AD_ID = \"AdId\"\n",
    "AGE = \"Age\"\n",
    "ZIP_CODE = \"CapZipCode\"\n",
    "COUNTRIES_VISITED = \"Countriesvisited\"\n",
    "FAVE_SPORTS = \"FaveSports\"\n",
    "GENDER = \"Gender\"\n",
    "HOME_COUNTRY = \"Homecountry\"\n",
    "HOME_TOWN = \"Hometown\"\n",
    "INCOME = \"Income\"\n",
    "LAST_NAME = \"LastName\"\n",
    "MOST_LISTENED_MUSICS = \"Mostlistenedmusics\"\n",
    "MOST_READ_BOOKS = \"Mostreadbooks\"\n",
    "MOST_VISITED_WEBSITES = \"Mostvisitedwebsites\"\n",
    "MOST_WATCHED_MOVIES = \"Mostwatchedmovies\"\n",
    "MOST_WATCHED_TV_PROGRAMMES = \"Mostwatchedtvprogrammes\"\n",
    "NAME = \"Name\"\n",
    "PAYPAL = \"Paypal\"\n",
    "TIMEPASS = \"Timepass\"\n",
    "TYPE_OF_JOB = \"TypeofJob\"\n",
    "WEEKLY_WORKING_HOURS = \"Weeklyworkinghours\"\n",
    "ADFILEPATH = \"AdFilePath\"\n",
    "GENDER_F = \"Gender_F\"\n",
    "GENDER_M = \"Gender_M\"\n",
    "# HomeCountry = 12 Columns\n",
    "HOMECOUNTRY_CANADA = \"Homecountry_Canada\"\n",
    "HOMECOUNTRY_CZECHREPUBLIC = \"Homecountry_CzechRepublic\"\n",
    "HOMECOUNTRY_GREATBRITAIN = \"Homecountry_GreatBritain\"\n",
    "HOMECOUNTRY_INDIA = \"Homecountry_India\"\n",
    "HOMECOUNTRY_ITALY = \"Homecountry_Italy\"\n",
    "HOMECOUNTRY_PHILLIPINES = \"Homecountry_Phillipines\"\n",
    "HOMECOUNTRY_ROMANIA = \"Homecountry_Romania\"\n",
    "HOMECOUNTRY_SAUDIARABIA = \"Homecountry_SaudiArabia\"\n",
    "HOMECOUNTRY_SINGAPORE = \"Homecountry_Singapore\"\n",
    "HOMECOUNTRY_SLOVENIA = \"Homecountry_Slovenia\"\n",
    "HOMECOUNTRY_UNITEDKINGDOM = \"Homecountry_UnitedKingdom\"\n",
    "HOMECOUNTRY_UNITEDSTATESOFAMERICA = \"Homecountry_UnitedStatesofAmerica\"\n",
    "# Income = 4 Columns\n",
    "INCOME_0 = \"Income_0\"\n",
    "INCOME_1 = \"Income_1\"\n",
    "INCOME_2 = \"Income_2\"\n",
    "INCOME_3 = \"Income_3\"\n",
    "# Mostlistenedmusics = 22 Columns\n",
    "MOSTLISTENEDMUSICS_1 = \"AlternativeMusic\"\n",
    "MOSTLISTENEDMUSICS_2 = \"AsianPopJPoporKpop\"\n",
    "MOSTLISTENEDMUSICS_3 = \"Blues\"\n",
    "MOSTLISTENEDMUSICS_4 = \"ClassicalMusic\"\n",
    "MOSTLISTENEDMUSICS_5 = \"CountryMusic\"\n",
    "MOSTLISTENEDMUSICS_6 = \"DanceMusic\"\n",
    "MOSTLISTENEDMUSICS_7 = \"EasyListening\"\n",
    "MOSTLISTENEDMUSICS_8 = \"ElectronicMusic\"\n",
    "MOSTLISTENEDMUSICS_9 = \"EuropeanMusicFolkPop\"\n",
    "MOSTLISTENEDMUSICS_10 = \"HipHopRap\"\n",
    "MOSTLISTENEDMUSICS_11 = \"IndiePop\"\n",
    "MOSTLISTENEDMUSICS_12 = \"InspirationalinclGospel\"\n",
    "MOSTLISTENEDMUSICS_13 = \"Jazz\"\n",
    "MOSTLISTENEDMUSICS_14 = \"LatinMusic\"\n",
    "MOSTLISTENEDMUSICS_15 = \"NewAge\"\n",
    "MOSTLISTENEDMUSICS_16 = \"Opera\"\n",
    "MOSTLISTENEDMUSICS_17 = \"PopPopularmusic\"\n",
    "MOSTLISTENEDMUSICS_18 = \"RampBSoul\"\n",
    "MOSTLISTENEDMUSICS_19 = \"Reggae\"\n",
    "MOSTLISTENEDMUSICS_20 = \"Rock\"\n",
    "MOSTLISTENEDMUSICS_21 = \"SingerSongwriterincFolk\"\n",
    "MOSTLISTENEDMUSICS_22 = \"WorldMusicBeats\"\n",
    "# Mostreadbooks = 31 Columns\n",
    "MOSTREADBOOKS_1 = \"ActionandAdventure\"\n",
    "MOSTREADBOOKS_2 = \"Anthologies\"\n",
    "MOSTREADBOOKS_3 = \"Art\"\n",
    "MOSTREADBOOKS_4 = \"Autobiographies\"\n",
    "MOSTREADBOOKS_5 = \"Biographies\"\n",
    "MOSTREADBOOKS_6 = \"Childrens\"\n",
    "MOSTREADBOOKS_7 = \"Childrensliterature\"\n",
    "MOSTREADBOOKS_8 = \"Comics\"\n",
    "MOSTREADBOOKS_9 = \"Cookbooks\"\n",
    "MOSTREADBOOKS_10 = \"Diaries\"\n",
    "MOSTREADBOOKS_11 = \"Drama\"\n",
    "MOSTREADBOOKS_12 = \"Encyclopedias\"\n",
    "MOSTREADBOOKS_13 = \"Eroticfiction\"\n",
    "MOSTREADBOOKS_14 = \"Fantasy\"\n",
    "MOSTREADBOOKS_15 = \"Guide\"\n",
    "MOSTREADBOOKS_16 = \"History\"\n",
    "MOSTREADBOOKS_17 = \"Horror\"\n",
    "MOSTREADBOOKS_18 = \"Journals\"\n",
    "MOSTREADBOOKS_19 = \"Math\"\n",
    "MOSTREADBOOKS_20 = \"Mystery\"\n",
    "MOSTREADBOOKS_21 = \"Poetry\"\n",
    "MOSTREADBOOKS_22 = \"Prayerbooks\"\n",
    "MOSTREADBOOKS_23 = \"Religious\"\n",
    "MOSTREADBOOKS_24 = \"Romance\"\n",
    "MOSTREADBOOKS_25 = \"Satire\"\n",
    "MOSTREADBOOKS_26 = \"Science\"\n",
    "MOSTREADBOOKS_27 = \"Sciencefiction\"\n",
    "MOSTREADBOOKS_28 = \"Selfhelp\"\n",
    "MOSTREADBOOKS_29 = \"Series\"\n",
    "MOSTREADBOOKS_30 = \"Travel\"\n",
    "MOSTREADBOOKS_31 = \"Trilogies\"\n",
    "# Mostwatchedmovies = 21 Columns\n",
    "MOSTWATCHEDMOVIES_1 = \"Mostwatchedmovies_Action\"\n",
    "MOSTWATCHEDMOVIES_2 = \"Mostwatchedmovies_Adventure\"\n",
    "MOSTWATCHEDMOVIES_3 = \"Mostwatchedmovies_Animation\"\n",
    "MOSTWATCHEDMOVIES_4 = \"Mostwatchedmovies_Biography\"\n",
    "MOSTWATCHEDMOVIES_5 = \"Mostwatchedmovies_Comedy\"\n",
    "MOSTWATCHEDMOVIES_6 = \"Mostwatchedmovies_CrimeandGangster\"\n",
    "MOSTWATCHEDMOVIES_7 = \"Mostwatchedmovies_Documentary\"\n",
    "MOSTWATCHEDMOVIES_8 = \"Mostwatchedmovies_Drama\"\n",
    "MOSTWATCHEDMOVIES_9 = \"Mostwatchedmovies_EpicHistorical\"\n",
    "MOSTWATCHEDMOVIES_10 = \"Mostwatchedmovies_Erotic\"\n",
    "MOSTWATCHEDMOVIES_11 = \"Mostwatchedmovies_Family\"\n",
    "MOSTWATCHEDMOVIES_12 = \"Mostwatchedmovies_Fantasy\"\n",
    "MOSTWATCHEDMOVIES_13 = \"Mostwatchedmovies_Horror\"\n",
    "MOSTWATCHEDMOVIES_14 = \"Mostwatchedmovies_Musical\"\n",
    "MOSTWATCHEDMOVIES_15 = \"Mostwatchedmovies_Mystery\"\n",
    "MOSTWATCHEDMOVIES_16 = \"Mostwatchedmovies_Romance\"\n",
    "MOSTWATCHEDMOVIES_17 = \"Mostwatchedmovies_SciFi\"\n",
    "MOSTWATCHEDMOVIES_18 = \"Mostwatchedmovies_Sport\"\n",
    "MOSTWATCHEDMOVIES_19 = \"Mostwatchedmovies_Thriller\"\n",
    "MOSTWATCHEDMOVIES_20 = \"Mostwatchedmovies_War\"\n",
    "MOSTWATCHEDMOVIES_21 = \"Mostwatchedmovies_Western\"\n",
    "# Mostwatchedtvprogrammes = 11 Columns\n",
    "MOSTWATCHEDTVPROGRAMMES_1 = \"Mostwatchedtvprogrammes_Childrens\"\n",
    "MOSTWATCHEDTVPROGRAMMES_2 = \"Mostwatchedtvprogrammes_Comedy\"\n",
    "MOSTWATCHEDTVPROGRAMMES_3 = \"Mostwatchedtvprogrammes_Drama\"\n",
    "MOSTWATCHEDTVPROGRAMMES_4 = \"Mostwatchedtvprogrammes_EntertainmentVarietyShows\"\n",
    "MOSTWATCHEDTVPROGRAMMES_5 = \"Mostwatchedtvprogrammes_Factual\"\n",
    "MOSTWATCHEDTVPROGRAMMES_6 = \"Mostwatchedtvprogrammes_Learning\"\n",
    "MOSTWATCHEDTVPROGRAMMES_7 = \"Mostwatchedtvprogrammes_Music\"\n",
    "MOSTWATCHEDTVPROGRAMMES_8 = \"Mostwatchedtvprogrammes_News\"\n",
    "MOSTWATCHEDTVPROGRAMMES_9 = \"Mostwatchedtvprogrammes_ReligionampEthics\"\n",
    "MOSTWATCHEDTVPROGRAMMES_10 = \"Mostwatchedtvprogrammes_Sport\"\n",
    "MOSTWATCHEDTVPROGRAMMES_11 = \"Mostwatchedtvprogrammes_Weather\"\n",
    "\n",
    "RATING = \"Rating\"\n",
    "AD_NUM_FACES = \"ad_num_faces\"\n",
    "AD_LABEL_FEATURE_1 = 'ad_isAdvertising'\n",
    "AD_LABEL_FEATURE_2 = 'ad_isBrand'\n",
    "AD_LABEL_FEATURE_3 = 'ad_isElectronicdevice'\n",
    "AD_LABEL_FEATURE_4 = 'ad_isElectronics'\n",
    "AD_LABEL_FEATURE_5 = 'ad_isFashionaccessory'\n",
    "AD_LABEL_FEATURE_6 = 'ad_isFictionalcharacter'\n",
    "AD_LABEL_FEATURE_7 = 'ad_isFont'\n",
    "AD_LABEL_FEATURE_8 = 'ad_isFurniture'\n",
    "AD_LABEL_FEATURE_9 = 'ad_isGadget'\n",
    "AD_LABEL_FEATURE_10 = 'ad_isGames'\n",
    "AD_LABEL_FEATURE_11 = 'ad_isGraphicdesign'\n",
    "AD_LABEL_FEATURE_12 = 'ad_isGraphics'\n",
    "AD_LABEL_FEATURE_13 = 'ad_isJewellery'\n",
    "AD_LABEL_FEATURE_14 = 'ad_isLine'\n",
    "AD_LABEL_FEATURE_15 = 'ad_isLogo'\n",
    "AD_LABEL_FEATURE_16 = 'ad_isMagenta'\n",
    "AD_LABEL_FEATURE_17 = 'ad_isMaterialproperty'\n",
    "AD_LABEL_FEATURE_18 = 'ad_isMultimedia'\n",
    "AD_LABEL_FEATURE_19 = 'ad_isProduct'\n",
    "AD_LABEL_FEATURE_20 = 'ad_isRectangle'\n",
    "AD_LABEL_FEATURE_21 = 'ad_isSkin'\n",
    "AD_LABEL_FEATURE_22 = 'ad_isTechnology'\n",
    "AD_LABEL_FEATURE_23 = 'ad_isText'\n",
    "AD_LABEL_FEATURE_24 = 'ad_isVehicle'\n",
    "AD_LABEL_FEATURE_25 = 'ad_isYellow'\n",
    "AD_SAFESEARCH_FEATURE_1 = 'ad_isAdult_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_2 ='ad_isAdult_VERY_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_3 ='ad_isSpoof_POSSIBLE'\n",
    "AD_SAFESEARCH_FEATURE_4 ='ad_isSpoof_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_5 ='ad_isSpoof_VERY_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_6 ='ad_isMedical_POSSIBLE'\n",
    "AD_SAFESEARCH_FEATURE_7 ='ad_isMedical_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_8 ='ad_isMedical_VERY_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_9 ='ad_isViolence_VERY_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_10 ='ad_isRacy_POSSIBLE'\n",
    "AD_SAFESEARCH_FEATURE_11 ='ad_isRacy_UNLIKELY'\n",
    "AD_SAFESEARCH_FEATURE_12 ='ad_isRacy_VERY_LIKELY'\n",
    "AD_SAFESEARCH_FEATURE_13 ='ad_isRacy_VERY_UNLIKELY'\n",
    "AD_OBJECT_FEATURE_1 = 'ad_isAnimal'\n",
    "AD_OBJECT_FEATURE_2 ='ad_isBelt'\n",
    "AD_OBJECT_FEATURE_3 ='ad_isBottle'\n",
    "AD_OBJECT_FEATURE_4 ='ad_isBox'\n",
    "AD_OBJECT_FEATURE_5 ='ad_isCameralens'\n",
    "AD_OBJECT_FEATURE_6 ='ad_isChair'\n",
    "AD_OBJECT_FEATURE_7 ='ad_isClothing'\n",
    "AD_OBJECT_FEATURE_8 ='ad_isEarrings'\n",
    "AD_OBJECT_FEATURE_9 ='ad_isFood'\n",
    "AD_OBJECT_FEATURE_10 ='ad_isHat'\n",
    "AD_OBJECT_FEATURE_11 ='ad_isLuggagebags'\n",
    "AD_OBJECT_FEATURE_12 ='ad_isMobilephone'\n",
    "AD_OBJECT_FEATURE_13 ='ad_isNecklace'\n",
    "AD_OBJECT_FEATURE_14 ='ad_isPackagedgoods'\n",
    "AD_OBJECT_FEATURE_15 ='ad_isPants'\n",
    "AD_OBJECT_FEATURE_16 ='ad_isPen'\n",
    "AD_OBJECT_FEATURE_17 ='ad_isPerson'\n",
    "AD_OBJECT_FEATURE_18 ='ad_isPillow'\n",
    "AD_OBJECT_FEATURE_19 ='ad_isPoster'\n",
    "AD_OBJECT_FEATURE_20 ='ad_isShoe'\n",
    "AD_OBJECT_FEATURE_21 ='ad_isTop'\n",
    "AD_OBJECT_FEATURE_22 ='ad_isToy'\n",
    "AD_OBJECT_FEATURE_23 ='ad_isWatch'\n",
    "AD_OBJECT_FEATURE_24  ='ad_isWheel'\n",
    "FAV = 'fav'\n",
    "UNFAV = 'unfav'\n",
    "\n",
    "\n",
    "# Read all columns as strings to avoid any errors\n",
    "COL_DEFAULTS = {\n",
    "    USER_ID: \"**\",\n",
    "    AD_ID: \"**\",\n",
    "    AGE: \"**\",\n",
    "    ZIP_CODE: \"**\",\n",
    "    COUNTRIES_VISITED: \"**\",\n",
    "    FAVE_SPORTS: \"**\",\n",
    "    GENDER: \"**\",\n",
    "    HOME_COUNTRY: \"**\",\n",
    "    HOME_TOWN: \"**\",\n",
    "    INCOME: \"**\",\n",
    "    LAST_NAME: \"**\",\n",
    "    MOST_LISTENED_MUSICS: \"**\",\n",
    "    MOST_READ_BOOKS: \"**\",\n",
    "    MOST_VISITED_WEBSITES: \"**\",\n",
    "    MOST_WATCHED_MOVIES: \"**\",\n",
    "    MOST_WATCHED_TV_PROGRAMMES: \"**\",\n",
    "    NAME: \"**\",\n",
    "    PAYPAL: \"**\",\n",
    "    TIMEPASS: \"**\",\n",
    "    TYPE_OF_JOB: \"**\",\n",
    "    WEEKLY_WORKING_HOURS: \"**\",\n",
    "    ADFILEPATH: \"**\",\n",
    "    GENDER_F: \"**\",\n",
    "    GENDER_M: \"**\",\n",
    "    HOMECOUNTRY_CANADA: \"**\",\n",
    "    HOMECOUNTRY_CZECHREPUBLIC: \"**\",\n",
    "    HOMECOUNTRY_GREATBRITAIN: \"**\",\n",
    "    HOMECOUNTRY_INDIA: \"**\",\n",
    "    HOMECOUNTRY_ITALY: \"**\",\n",
    "    HOMECOUNTRY_PHILLIPINES: \"**\",\n",
    "    HOMECOUNTRY_ROMANIA: \"**\",\n",
    "    HOMECOUNTRY_SAUDIARABIA: \"**\",\n",
    "    HOMECOUNTRY_SINGAPORE: \"**\",\n",
    "    HOMECOUNTRY_SLOVENIA: \"**\",\n",
    "    HOMECOUNTRY_UNITEDKINGDOM: \"**\",\n",
    "    HOMECOUNTRY_UNITEDSTATESOFAMERICA: \"**\",\n",
    "    INCOME_0: \"**\",\n",
    "    INCOME_1: \"**\",\n",
    "    INCOME_2: \"**\",\n",
    "    INCOME_3: \"**\",\n",
    "    MOSTLISTENEDMUSICS_1: \"**\",\n",
    "    MOSTLISTENEDMUSICS_2: \"**\",\n",
    "    MOSTLISTENEDMUSICS_3: \"**\",\n",
    "    MOSTLISTENEDMUSICS_4: \"**\",\n",
    "    MOSTLISTENEDMUSICS_5: \"**\",\n",
    "    MOSTLISTENEDMUSICS_6: \"**\",\n",
    "    MOSTLISTENEDMUSICS_7: \"**\",\n",
    "    MOSTLISTENEDMUSICS_8: \"**\",\n",
    "    MOSTLISTENEDMUSICS_9: \"**\",\n",
    "    MOSTLISTENEDMUSICS_10: \"**\",\n",
    "    MOSTLISTENEDMUSICS_11: \"**\",\n",
    "    MOSTLISTENEDMUSICS_12: \"**\",\n",
    "    MOSTLISTENEDMUSICS_13: \"**\",\n",
    "    MOSTLISTENEDMUSICS_14: \"**\",\n",
    "    MOSTLISTENEDMUSICS_15: \"**\",\n",
    "    MOSTLISTENEDMUSICS_16: \"**\",\n",
    "    MOSTLISTENEDMUSICS_17: \"**\",\n",
    "    MOSTLISTENEDMUSICS_18: \"**\",\n",
    "    MOSTLISTENEDMUSICS_19: \"**\",\n",
    "    MOSTLISTENEDMUSICS_20: \"**\",\n",
    "    MOSTLISTENEDMUSICS_21: \"**\",\n",
    "    MOSTLISTENEDMUSICS_22: \"**\",\n",
    "    MOSTREADBOOKS_1: \"**\",\n",
    "    MOSTREADBOOKS_2: \"**\",\n",
    "    MOSTREADBOOKS_3: \"**\",\n",
    "    MOSTREADBOOKS_4: \"**\",\n",
    "    MOSTREADBOOKS_5: \"**\",\n",
    "    MOSTREADBOOKS_6: \"**\",\n",
    "    MOSTREADBOOKS_7: \"**\",\n",
    "    MOSTREADBOOKS_8: \"**\",\n",
    "    MOSTREADBOOKS_9: \"**\",\n",
    "    MOSTREADBOOKS_10: \"**\",\n",
    "    MOSTREADBOOKS_11: \"**\",\n",
    "    MOSTREADBOOKS_12: \"**\",\n",
    "    MOSTREADBOOKS_13: \"**\",\n",
    "    MOSTREADBOOKS_14: \"**\",\n",
    "    MOSTREADBOOKS_15: \"**\",\n",
    "    MOSTREADBOOKS_16: \"**\",\n",
    "    MOSTREADBOOKS_17: \"**\",\n",
    "    MOSTREADBOOKS_18: \"**\",\n",
    "    MOSTREADBOOKS_19: \"**\",\n",
    "    MOSTREADBOOKS_20: \"**\",\n",
    "    MOSTREADBOOKS_21: \"**\",\n",
    "    MOSTREADBOOKS_22: \"**\",\n",
    "    MOSTREADBOOKS_23: \"**\",\n",
    "    MOSTREADBOOKS_24: \"**\",\n",
    "    MOSTREADBOOKS_25: \"**\",\n",
    "    MOSTREADBOOKS_26: \"**\",\n",
    "    MOSTREADBOOKS_27: \"**\",\n",
    "    MOSTREADBOOKS_28: \"**\",\n",
    "    MOSTREADBOOKS_29: \"**\",\n",
    "    MOSTREADBOOKS_30: \"**\",\n",
    "    MOSTREADBOOKS_31: \"**\",\n",
    "    MOSTWATCHEDMOVIES_1: \"**\",\n",
    "    MOSTWATCHEDMOVIES_2: \"**\",\n",
    "    MOSTWATCHEDMOVIES_3: \"**\",\n",
    "    MOSTWATCHEDMOVIES_4: \"**\",\n",
    "    MOSTWATCHEDMOVIES_5: \"**\",\n",
    "    MOSTWATCHEDMOVIES_6: \"**\",\n",
    "    MOSTWATCHEDMOVIES_7: \"**\",\n",
    "    MOSTWATCHEDMOVIES_8: \"**\",\n",
    "    MOSTWATCHEDMOVIES_9: \"**\",\n",
    "    MOSTWATCHEDMOVIES_10: \"**\",\n",
    "    MOSTWATCHEDMOVIES_11: \"**\",\n",
    "    MOSTWATCHEDMOVIES_12: \"**\",\n",
    "    MOSTWATCHEDMOVIES_13: \"**\",\n",
    "    MOSTWATCHEDMOVIES_14: \"**\",\n",
    "    MOSTWATCHEDMOVIES_15: \"**\",\n",
    "    MOSTWATCHEDMOVIES_16: \"**\",\n",
    "    MOSTWATCHEDMOVIES_17: \"**\",\n",
    "    MOSTWATCHEDMOVIES_18: \"**\",\n",
    "    MOSTWATCHEDMOVIES_19: \"**\",\n",
    "    MOSTWATCHEDMOVIES_20: \"**\",\n",
    "    MOSTWATCHEDMOVIES_21: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_1: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_2: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_3: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_4: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_5: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_6: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_7: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_8: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_9: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_10: \"**\",\n",
    "    MOSTWATCHEDTVPROGRAMMES_11: \"**\",    \n",
    "    RATING: \"**\",\n",
    "    AD_NUM_FACES: \"**\",\n",
    "    FAV: \"**\",\n",
    "    UNFAV: \"**\"\n",
    "}\n",
    "\n",
    "\n",
    "AD_FACE_COLS = [AD_NUM_FACES]\n",
    "AD_LABEL_COLS = [AD_LABEL_FEATURE_1,AD_LABEL_FEATURE_2,AD_LABEL_FEATURE_3,AD_LABEL_FEATURE_4,AD_LABEL_FEATURE_5,\n",
    "                AD_LABEL_FEATURE_6,AD_LABEL_FEATURE_7,AD_LABEL_FEATURE_8,AD_LABEL_FEATURE_9,AD_LABEL_FEATURE_10,\n",
    "                AD_LABEL_FEATURE_11,AD_LABEL_FEATURE_12,AD_LABEL_FEATURE_13,AD_LABEL_FEATURE_14,AD_LABEL_FEATURE_15,\n",
    "                AD_LABEL_FEATURE_16,AD_LABEL_FEATURE_17,AD_LABEL_FEATURE_18,AD_LABEL_FEATURE_19,AD_LABEL_FEATURE_20,\n",
    "                AD_LABEL_FEATURE_21,AD_LABEL_FEATURE_22,AD_LABEL_FEATURE_23,AD_LABEL_FEATURE_24,AD_LABEL_FEATURE_25]\n",
    "\n",
    "AD_OBJECT_COLS = [AD_OBJECT_FEATURE_1,AD_OBJECT_FEATURE_2,AD_OBJECT_FEATURE_3,AD_OBJECT_FEATURE_4,AD_OBJECT_FEATURE_5,\n",
    "                AD_OBJECT_FEATURE_6,AD_OBJECT_FEATURE_7,AD_OBJECT_FEATURE_8,AD_OBJECT_FEATURE_9,AD_OBJECT_FEATURE_10,\n",
    "                AD_OBJECT_FEATURE_11,AD_OBJECT_FEATURE_12,AD_OBJECT_FEATURE_13,AD_OBJECT_FEATURE_14,AD_OBJECT_FEATURE_15,\n",
    "                AD_OBJECT_FEATURE_16,AD_OBJECT_FEATURE_17,AD_OBJECT_FEATURE_18,AD_OBJECT_FEATURE_19,AD_OBJECT_FEATURE_20,\n",
    "                AD_OBJECT_FEATURE_21,AD_OBJECT_FEATURE_22,AD_OBJECT_FEATURE_23,AD_OBJECT_FEATURE_24]\n",
    "\n",
    "\n",
    "AD_SAFE_SEARCH_COLS = [AD_SAFESEARCH_FEATURE_1,AD_SAFESEARCH_FEATURE_2,AD_SAFESEARCH_FEATURE_3,AD_SAFESEARCH_FEATURE_4,\n",
    "                      AD_SAFESEARCH_FEATURE_5,AD_SAFESEARCH_FEATURE_6,AD_SAFESEARCH_FEATURE_7,AD_SAFESEARCH_FEATURE_8,\n",
    "                      AD_SAFESEARCH_FEATURE_9,AD_SAFESEARCH_FEATURE_10,AD_SAFESEARCH_FEATURE_11,AD_SAFESEARCH_FEATURE_12,AD_SAFESEARCH_FEATURE_13]\n",
    "\n",
    "\n",
    "SELECTED_AD_COLS = AD_FACE_COLS  + AD_LABEL_COLS + AD_OBJECT_COLS + AD_SAFE_SEARCH_COLS\n",
    "\n",
    "SELECTED_HOMECOUNTRY_COLS = [HOMECOUNTRY_CANADA, HOMECOUNTRY_CZECHREPUBLIC, HOMECOUNTRY_GREATBRITAIN,\n",
    "                             HOMECOUNTRY_INDIA, HOMECOUNTRY_ITALY, HOMECOUNTRY_PHILLIPINES, HOMECOUNTRY_ROMANIA,\n",
    "                             HOMECOUNTRY_SAUDIARABIA, HOMECOUNTRY_SINGAPORE, HOMECOUNTRY_SLOVENIA,\n",
    "                             HOMECOUNTRY_UNITEDKINGDOM, HOMECOUNTRY_UNITEDSTATESOFAMERICA]\n",
    "\n",
    "SELECTED_INCOME_COLS = [INCOME_0, INCOME_1, INCOME_2, INCOME_3]\n",
    "\n",
    "SELECTED_MOSTLISTENEDMUSICS_COLS = [MOSTLISTENEDMUSICS_1, MOSTLISTENEDMUSICS_2, MOSTLISTENEDMUSICS_3,\n",
    "                                    MOSTLISTENEDMUSICS_4, MOSTLISTENEDMUSICS_5, MOSTLISTENEDMUSICS_6,\n",
    "                                    MOSTLISTENEDMUSICS_7, MOSTLISTENEDMUSICS_8, MOSTLISTENEDMUSICS_9,\n",
    "                                    MOSTLISTENEDMUSICS_10, MOSTLISTENEDMUSICS_11, MOSTLISTENEDMUSICS_12,\n",
    "                                    MOSTLISTENEDMUSICS_13, MOSTLISTENEDMUSICS_14, MOSTLISTENEDMUSICS_15,\n",
    "                                    MOSTLISTENEDMUSICS_16, MOSTLISTENEDMUSICS_17, MOSTLISTENEDMUSICS_18,\n",
    "                                    MOSTLISTENEDMUSICS_19, MOSTLISTENEDMUSICS_20, MOSTLISTENEDMUSICS_21,\n",
    "                                    MOSTLISTENEDMUSICS_22]\n",
    "\n",
    "SELECTED_MOSTREADBOOKS_COLS = [MOSTREADBOOKS_1, MOSTREADBOOKS_2, MOSTREADBOOKS_3, MOSTREADBOOKS_4,\n",
    "                               MOSTREADBOOKS_5, MOSTREADBOOKS_6, MOSTREADBOOKS_7, MOSTREADBOOKS_8,\n",
    "                               MOSTREADBOOKS_9, MOSTREADBOOKS_10, MOSTREADBOOKS_11, MOSTREADBOOKS_12,\n",
    "                               MOSTREADBOOKS_13, MOSTREADBOOKS_14, MOSTREADBOOKS_15, MOSTREADBOOKS_16,\n",
    "                               MOSTREADBOOKS_17, MOSTREADBOOKS_18, MOSTREADBOOKS_19, MOSTREADBOOKS_20,\n",
    "                               MOSTREADBOOKS_21, MOSTREADBOOKS_22, MOSTREADBOOKS_23, MOSTREADBOOKS_24,\n",
    "                               MOSTREADBOOKS_25, MOSTREADBOOKS_26, MOSTREADBOOKS_27, MOSTREADBOOKS_28,\n",
    "                               MOSTREADBOOKS_29, MOSTREADBOOKS_30, MOSTREADBOOKS_31] \n",
    "\n",
    "SELECTED_MOSTWATCHEDMOVIES_COLS = [MOSTWATCHEDMOVIES_1, MOSTWATCHEDMOVIES_2, MOSTWATCHEDMOVIES_3,\n",
    "                                   MOSTWATCHEDMOVIES_4, MOSTWATCHEDMOVIES_5, MOSTWATCHEDMOVIES_6,\n",
    "                                   MOSTWATCHEDMOVIES_7, MOSTWATCHEDMOVIES_8, MOSTWATCHEDMOVIES_9,\n",
    "                                   MOSTWATCHEDMOVIES_10, MOSTWATCHEDMOVIES_11, MOSTWATCHEDMOVIES_12,\n",
    "                                   MOSTWATCHEDMOVIES_13, MOSTWATCHEDMOVIES_14, MOSTWATCHEDMOVIES_15,\n",
    "                                   MOSTWATCHEDMOVIES_16, MOSTWATCHEDMOVIES_17, MOSTWATCHEDMOVIES_18,\n",
    "                                   MOSTWATCHEDMOVIES_19, MOSTWATCHEDMOVIES_20, MOSTWATCHEDMOVIES_21]\n",
    "\n",
    "SELECTED_MOSTWATCHEDTVPROGRAMMES_COLS = [MOSTWATCHEDTVPROGRAMMES_1, MOSTWATCHEDTVPROGRAMMES_2,\n",
    "                                         MOSTWATCHEDTVPROGRAMMES_3, MOSTWATCHEDTVPROGRAMMES_4,\n",
    "                                         MOSTWATCHEDTVPROGRAMMES_5, MOSTWATCHEDTVPROGRAMMES_6,\n",
    "                                         MOSTWATCHEDTVPROGRAMMES_7, MOSTWATCHEDTVPROGRAMMES_8,\n",
    "                                         MOSTWATCHEDTVPROGRAMMES_9, MOSTWATCHEDTVPROGRAMMES_10,\n",
    "                                         MOSTWATCHEDTVPROGRAMMES_11]\n",
    "                                   \n",
    "SELECTED_INP_COLS = [AGE, ZIP_CODE, FAVE_SPORTS, GENDER_F, GENDER_M] +\\\n",
    "                    SELECTED_AD_COLS +\\\n",
    "                    SELECTED_HOMECOUNTRY_COLS +\\\n",
    "                    SELECTED_INCOME_COLS +\\\n",
    "                    SELECTED_MOSTLISTENEDMUSICS_COLS +\\\n",
    "                    SELECTED_MOSTREADBOOKS_COLS +\\\n",
    "                    SELECTED_MOSTWATCHEDMOVIES_COLS +\\\n",
    "                    SELECTED_MOSTWATCHEDTVPROGRAMMES_COLS\n",
    "\n",
    "EMBED_COLS = [FAV, UNFAV]\n",
    "\n",
    "SELECTED_COLS = SELECTED_INP_COLS + [TARGET_COL]\n",
    "\n",
    "print(SELECTED_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ad_dataset_pd(usecols:List[str]):\n",
    "    \"\"\"\n",
    "    Read from csv files given set of columns into Pandas Dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(users_ads_rating_csv,usecols=usecols, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ad_dataset_pd(SELECTED_COLS).sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_VEC_DIMENSIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   6% ||                                      | ETA:   0:01:31   8.4 MiB/s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Projects/responsible_ai_hackathon/models/basic-model/EmbeddingFactory.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_folder, embedding_name, embed_dim, embed_zip_folder, **read_csv_kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_zip_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mread_csv_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/responsible_ai_hackathon/models/basic-model/EmbeddingFactory.py\u001b[0m in \u001b[0;36m_prepare_embeddings\u001b[0;34m(self, embedding_name, embed_dim, embed_zip_folder, **read_csv_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0membed_zip_folder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find {embedding_name} locally. Started download.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mdownload_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchakin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_zip_folder\u001b[0m  \u001b[0;31m# Use the downloaded folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/site-packages/chakin/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(number, name, save_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdlProgress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_index = EmbeddingFactory(Path(\"./embeddings/\"), \"GloVe.6B.50d\", WORD_VEC_DIMENSIONS, nrows=None, skiprows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_project(d:Dict, cols:List[str]) -> Dict:\n",
    "    \"\"\"Returns a new dictionary with only cols keys\"\"\"\n",
    "    return {k:v for k, v in d.items() if k in cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexerForVocab:\n",
    "    def __init__(self, vocab_list:List[str], oov_index:int=0):\n",
    "        \"\"\"\n",
    "        Creates a string indexer for the vocabulary with out of vocabulary (oov) indexing\n",
    "        \"\"\"\n",
    "        self._vocab_map = {v:i+1 for i, v in enumerate(vocab_list)}\n",
    "        self._oov = oov_index\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Map for {len(self)} keys with 1 OOV key\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._vocab_map) + 1\n",
    "        \n",
    "    def index_of(self, item:str):\n",
    "        \"\"\"\n",
    "        Index of item in the vocabulary\n",
    "        \"\"\"\n",
    "        return self._vocab_map.get(item, self._oov)\n",
    "    \n",
    "    def index_of_mux(self, items:List[str]):\n",
    "        return [self.index_of(i) for i in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = defaultdict(dict) # Store matrix and metadata for each embedding column for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_embed_cols(df:pd.DataFrame, embed_col:str):\n",
    "    \"\"\"\n",
    "    Takes dataframe and column name and transforms column and stores\n",
    "    vocab size, max len & embedding matrix into CACHE\n",
    "    \"\"\"\n",
    "    # Input dataframe and embed_col\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(df[embed_col])\n",
    "\n",
    "    # UNK added as tokenizer starts indexing from 1\n",
    "    words = [\"UNK\"] + list(t.word_index.keys()) # in order of tokenizer\n",
    "    CACHE[embed_col][\"vocab_size\"] = len(words)\n",
    "\n",
    "    # integer encode the text data\n",
    "    encoded_col = t.texts_to_sequences(df[embed_col])\n",
    "\n",
    "    # calculate max len of vector and make length equal by padding with zeros\n",
    "    maxlen = max(len(x) for x in encoded_col) \n",
    "    CACHE[embed_col][\"maxlen\"] = maxlen\n",
    "\n",
    "    padded_col = pad_sequences(encoded_col, maxlen=maxlen, padding='post')\n",
    "\n",
    "    # create a weight matrix\n",
    "    embeddings = dict.fromkeys(words, \" \".join([\"0\"] * WORD_VEC_DIMENSIONS)) # default embeddings to all words as 0\n",
    "    embeddings.update(dict(embedding_index.fetch_word_vectors(words))) # update for known words\n",
    "    # reorder to match tokenizer's indexing\n",
    "    emb_matrix = pd.DataFrame.from_dict(embeddings, orient=\"index\").loc[words, 0].str.split(\" \", expand=True).to_numpy().astype(np.float16)\n",
    "    CACHE[embed_col][\"embed_matrix\"] = emb_matrix\n",
    "    assert emb_matrix.shape[0] == len(words), \"Not all words have embeddings\"\n",
    "    \n",
    "    return padded_col, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _ = transform_embed_cols(ad_dataset_pd([FAV]).sample(n=5), FAV)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE, CACHE[FAV][\"embed_matrix\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "Convert to a number and remove any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Tensorflow Data Validation APIs data-exploration/tensorflow-data-validation.ipynb\n",
    "\n",
    "MEAN_AGE, STD_AGE, MEDIAN_AGE, MAX_AGE = 31.74, 12.07, 29, 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_age(age_str:tf.string, default_age=MEDIAN_AGE) -> int:\n",
    "    \"\"\"Typecast age to an integer and update outliers with the default\"\"\"\n",
    "    try:\n",
    "        age = int(age_str)\n",
    "        if age < 0 or age > MAX_AGE:\n",
    "            raise ValueError(f\"{age} is not a valid age\")\n",
    "    except:\n",
    "        age = default_age\n",
    "    normalized_age = (age - MEAN_AGE) / STD_AGE\n",
    "    return normalized_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_age(\"50\"), fix_age(\"50.5\"), fix_age(\"-10\"), fix_age(\"bad_age_10\"), fix_age(\"300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Code\n",
    "\n",
    "Prepare zip-code column for one-hot encoding each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ZIP_CODE, FIRST_K_ZIP_DIGITS = \"00000\", 2\n",
    "\n",
    "zip_code_indexer = IndexerForVocab(string.digits + string.ascii_lowercase + string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_zip_code_tensor(zip_code:tf.string, n_digits, indexer) -> List[str]:\n",
    "    \"\"\"Extracts the the first n_digits as a list\"\"\"\n",
    "    zip_digits = []\n",
    "    try:\n",
    "        if isinstance(zip_code, tf.Tensor):\n",
    "            zip_code = zip_code.numpy()[0].decode('ascii', errors=\"ignore\") # very ineffecient way\n",
    "        zip_digits = list(zip_code.strip()[:n_digits])\n",
    "    except:\n",
    "        zip_digits = list(DEFAULT_ZIP_CODE[:n_digits])\n",
    "    return tf.concat( [\n",
    "        tf.one_hot(\n",
    "            indexer.index_of(d), len(indexer)\n",
    "        ) for d in zip_digits\n",
    "    ], 0 )\n",
    "\n",
    "def fix_zip_code(zip_code:str, n_digits, indexer) -> List[str]:\n",
    "    \"\"\"Extracts the the first n_digits as a list\"\"\"\n",
    "    zip_digits = []\n",
    "    try:\n",
    "        zip_digits = list(zip_code.strip()[:n_digits])\n",
    "    except:\n",
    "        zip_digits = list(DEFAULT_ZIP_CODE[:n_digits])\n",
    "    return np.ravel(np.eye(len(indexer))[indexer.index_of_mux(zip_digits)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zip_code_indexer = IndexerForVocab(string.digits)\n",
    "\n",
    "(fix_zip_code(\"43556\", 10, test_zip_code_indexer),\n",
    "fix_zip_code(\"43556\", 2, test_zip_code_indexer),\n",
    "fix_zip_code(\"43556\", 4, test_zip_code_indexer),\n",
    "fix_zip_code(None, 3, test_zip_code_indexer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Favorite Sports\n",
    "\n",
    "Two approaches,\n",
    "1. Consider the first `K` sports mentioned by each user and one-hot encode each separately\n",
    "2. Multi label binarize all the sports as there are only 15 unique sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAV_SPORTS_UNKNOWN = \"UNK_SPORT\"\n",
    "ALL_FAV_SPORTS = ['Olympic sports', 'Winter sports', 'Nothing', 'I do not like Sports', 'Equestrian sports', 'Skating sports', 'Precision sports', 'Hunting sports', 'Motor sports', 'Team sports', 'Individual sports', 'Other', 'Water sports', 'Indoor sports', 'Endurance sports']\n",
    "\n",
    "fav_sports_binarizer = MultiLabelBinarizer()\n",
    "fav_sports_binarizer.fit([ALL_FAV_SPORTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fav_sports_multi_select_str_to_list(sports_str:Union[str, tf.Tensor]) -> List[str]:\n",
    "    # remove commas that dont separate different user selections\n",
    "    # example, commas inside paranthesis of \"Individual sports (Tennis, Archery, ...)\" dont make new sports\n",
    "    if isinstance(sports_str, tf.Tensor):\n",
    "        sports_str = sports_str.numpy()[0].decode('ascii', errors=\"ignore\")\n",
    "    else:\n",
    "        sports_str = sports_str.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\") # remove non-ascii chars\n",
    "    sports = re.sub(r\"\\s*\\(.*,.*\\)\\s*\", \"\", sports_str)\n",
    "    return re.split(r\"\\s*,\\s*\", sports)\n",
    "\n",
    "def fix_fav_sports_mlb(sports_str:str) -> List[int]:\n",
    "    sports = fav_sports_multi_select_str_to_list(sports_str)\n",
    "    return fav_sports_binarizer.transform([sports])[0]\n",
    "\n",
    "def fix_fav_sports_firstk(sports_str:str, first_k:int, pad_constant:int) -> List[str]:\n",
    "    sports = fav_sports_multi_select_str_to_list(sports_str)\n",
    "    right_pad_width = first_k - len(sports_enc)\n",
    "    result = [sports + [pad_constant] * right_pad_width][:first_k]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    fix_fav_sports_mlb(\"Individual sports (Tennis, Archery, ...), Indoor sports, Endurance sports, Skating sports\"),\n",
    "    fix_fav_sports_mlb(\"Skating sports\"),\n",
    "    fix_fav_sports_mlb(\"Individual sports (Tennis, Archery, ...)\"),\n",
    "    fix_fav_sports_mlb(\"Indoor sports, Endurance sports, Skating sports\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_CARDINALITY = 5 # not zero based indexing i.e. ratings range from 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_pd(rating_str:str):\n",
    "    return np.eye(RATINGS_CARDINALITY, dtype=int)[int(float(rating_str)) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pd_X(df:pd.DataFrame, inp_cols:List[str]):\n",
    "    \"\"\"Original dataframe will be modified\"\"\"\n",
    "    df[AGE] = df[AGE].apply(lambda age: [fix_age(age)])\n",
    "    df[ZIP_CODE] = df[ZIP_CODE].apply(lambda zc: fix_zip_code(zc, n_digits=2, indexer=zip_code_indexer))\n",
    "    df[FAVE_SPORTS] = df[FAVE_SPORTS].apply(fix_fav_sports_mlb)\n",
    "\n",
    "    int_cols = [GENDER_F, GENDER_M, AD_NUM_FACES] +\\\n",
    "               AD_LABEL_COLS +\\\n",
    "               AD_SAFE_SEARCH_COLS +\\\n",
    "               SELECTED_HOMECOUNTRY_COLS +\\\n",
    "               SELECTED_INCOME_COLS +\\\n",
    "               SELECTED_MOSTLISTENEDMUSICS_COLS +\\\n",
    "               SELECTED_MOSTREADBOOKS_COLS +\\\n",
    "               SELECTED_MOSTWATCHEDMOVIES_COLS +\\\n",
    "               SELECTED_MOSTWATCHEDTVPROGRAMMES_COLS +\\\n",
    "               AD_OBJECT_COLS\n",
    "    \n",
    "    df[int_cols] = df[int_cols].applymap(lambda f: [int(f)])\n",
    "    \n",
    "    df[\"X\"] = df[inp_cols].apply(np.concatenate, axis=1)\n",
    "    # TODO: vectorize, else inefficient to sequentially loop over all examples\n",
    "    X = np.array([x for x in df[\"X\"]])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pd_y(df:pd.DataFrame, target_col:str):\n",
    "    \"\"\"Original dataframe will be modified\"\"\"\n",
    "    df[\"y\"] = df[target_col].apply(create_target_pd)\n",
    "    # TODO: vectorize, else inefficient to sequentially loop over all examples\n",
    "    y = np.array([y for y in df[\"y\"]])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_pd(inp_cols:List[str]=SELECTED_INP_COLS, target_col:str=TARGET_COL, fraction:float=1, test_frac:float=0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List]:\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training on a fraction of all input data\n",
    "    Columns using embeddings are split seperately and returned in list of tuples called embed_features\n",
    "    \"\"\"\n",
    "    # NOTE: RANDOM_SEED should be same for both splits\n",
    "    # Create (train, test) split of selected columns and target \n",
    "    df = ad_dataset_pd(SELECTED_COLS + EMBED_COLS).sample(frac=fraction)\n",
    "    \n",
    "    X, y = transform_pd_X(df[SELECTED_COLS], inp_cols), transform_pd_y(df[SELECTED_COLS], target_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Create (train, test) split for each embedding column\n",
    "    embed_features = {}\n",
    "    for embed_col in EMBED_COLS:\n",
    "        X_embed_col, tokenizer = transform_embed_cols(df, embed_col)\n",
    "        X_embed_train, X_embed_test = train_test_split(X_embed_col, test_size=test_frac, random_state=RANDOM_SEED)\n",
    "        embed_features[embed_col] = {\"train\": X_embed_train, \"test\": X_embed_test}\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test, embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Monitor training and other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "notebook.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Create a model and train using high level APIs like `tf.keras` and `tf.estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Z1eVQu9.png\" width=\"600\" height=\"300\">\n",
    "<p style=\"text-align: center;\"><strong>Image Credits:</strong> https://www.kaggle.com/colinmorris/embedding-layers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test, embed_features = create_dataset_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_metrics = [\n",
    "    \"accuracy\",\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc')\n",
    "]\n",
    "train_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of train cat & num features \",X_train.shape)\n",
    "print(\"Size of output for train \",y_train.shape)\n",
    "print(\"Size of test cat & num features \",X_test.shape)\n",
    "print(\"Size of output for test \",y_test.shape)\n",
    "print(\"No. of embedded features \",len(embed_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what CACHE contains and check it's shape\n",
    "CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_unknown_word_count(col:str):\n",
    "    unk_embed_vec = np.zeros((CACHE[col][\"vocab_size\"], WORD_VEC_DIMENSIONS))\n",
    "    num_rows = np.count_nonzero(CACHE[col][\"embed_matrix\"] == unk_embed_vec, axis=1).sum() / WORD_VEC_DIMENSIONS\n",
    "    logging.warning(f\"Could't find embeddings for {int(num_rows)} words in {col} column\")\n",
    "    \n",
    "log_unknown_word_count(FAV), log_unknown_word_count(UNFAV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embed_flat_layer(col:str, trainable_embed:bool=False):\n",
    "    col_input = Input(shape=(CACHE[col]['maxlen'],))\n",
    "    col_embedded = Embedding(CACHE[col]['vocab_size'], WORD_VEC_DIMENSIONS, weights=[CACHE[col]['embed_matrix']],\n",
    "                     input_length = CACHE[col]['maxlen'], trainable=trainable_embed)(col_input)\n",
    "    col_embedded_flat = Flatten()(col_embedded)\n",
    "    return col_input, col_embedded_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_embeddings():\n",
    "    # Input layers\n",
    "    selected_cols_input = Input(shape=(X_train.shape[1],))\n",
    "    fav_input, fav_embed = create_embed_flat_layer(FAV)\n",
    "    unfav_input, unfav_embed = create_embed_flat_layer(UNFAV)\n",
    "\n",
    "    # Concatenate the layers\n",
    "\n",
    "    concatenated = concatenate([fav_embed, unfav_embed, selected_cols_input]) \n",
    "    out = Dense(10, activation='relu')(concatenated)\n",
    "    out = Dense(RATINGS_CARDINALITY, activation='softmax')(out)\n",
    "\n",
    "    # Create the model\n",
    "    return Model(\n",
    "        inputs = [fav_input, unfav_input, selected_cols_input],\n",
    "        outputs = out,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_without_embeddings():\n",
    "    selected_cols_input = Input(shape=(X_train.shape[1],))\n",
    "    out = Dense(10, activation='relu')(selected_cols_input)\n",
    "    out = Dense(RATINGS_CARDINALITY, activation='softmax')(out)\n",
    "\n",
    "    # Create the model\n",
    "    return Model(\n",
    "        inputs = selected_cols_input,\n",
    "        outputs = out,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_data(with_embed:bool=True):\n",
    "    d = {}\n",
    "    if with_embed:\n",
    "        d[\"X_train\"] = [embed_features[FAV][\"train\"], embed_features[UNFAV][\"train\"], X_train]\n",
    "        d[\"y_train\"] = y_train\n",
    "        d[\"val_data\"] = ([embed_features[FAV][\"test\"], embed_features[UNFAV][\"test\"], X_test], y_test)\n",
    "    else:\n",
    "        d[\"X_train\"] = X_train\n",
    "        d[\"y_train\"] = y_train\n",
    "        d[\"val_data\"] = (X_test, y_test)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_without_embeddings()\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(\n",
    "        learning_rate=0.003,\n",
    "        clipvalue=0.5\n",
    "    ), \n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=keras_model_metrics\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Track hyperparameters using https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path(\"logs\")/datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    logdir, \n",
    "    histogram_freq=max(1, ceil(EPOCHS / 20)), # to control the amount of logging\n",
    "#     embeddings_freq=epochs,\n",
    ")\n",
    "print(f\"Logging tensorboard data at {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mfd = model_fit_data(with_embed=False)\n",
    "train_histories.append(model.fit(\n",
    "    mfd[\"X_train\"], mfd[\"y_train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=3,\n",
    "    validation_data=mfd[\"val_data\"],\n",
    "    callbacks=[tfdocs.modeling.EpochDots(), tensorboard_callback], \n",
    "    verbose=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(train_histories[-1].history) # pick the latest training history\n",
    "\n",
    "metrics_df.tail(1) # pick the last epoch's metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tip:` You can copy the final metrics row from above and paste it using `Shift + Cmd + V` in our [sheet](https://docs.google.com/spreadsheets/d/1v-nYiDA3elM1UP9stkB42MK0bTbuLxYJE7qAYDP8FHw/edit#gid=925421130) to accurately place all values in the respective columns\n",
    "\n",
    "**IMPORTANT**: Please don't forget to update git version ID column after you check-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics with p-value\n",
    "\n",
    "TODO: Run multiple times on different samples of `y_test` to compute p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, precision_score, recall_score, f1_score\n",
    "import sklearn\n",
    "from collections import OrderedDict\n",
    "\n",
    "assert sklearn.__version__.startswith('0.22'), \"Please upgrade scikit-learn (https://scikit-learn.org/stable/install.html)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict([embed_features[FAV][\"test\"], embed_features[UNFAV][\"test\"], X_test], BATCH_SIZE)\n",
    "y_true = y_test\n",
    "y_pred = (y_prob / np.max(y_prob, axis=1).reshape(-1, 1)).astype(int) # convert probabilities to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(OrderedDict({\n",
    "    \"macro_roc_auc_ovo\": [roc_auc_score(y_test, y_prob, multi_class=\"ovo\", average=\"macro\")],\n",
    "    \"weighted_roc_auc_ovo\": roc_auc_score(y_test, y_prob, multi_class=\"ovo\", average=\"weighted\"),\n",
    "    \"macro_roc_auc_ovr\": roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average=\"macro\"),\n",
    "    \"weighted_roc_auc_ovr\": roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average=\"weighted\"),\n",
    "    \"weighted_precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"weighted_recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"weighted_f1\": f1_score(y_test, y_pred, average=\"weighted\")\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also paste the above numbers to our [sheet](https://docs.google.com/spreadsheets/d/1v-nYiDA3elM1UP9stkB42MK0bTbuLxYJE7qAYDP8FHw/edit#gid=925421130&range=W1:AC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q -U tensorflow-model-analysis==0.21.6 apache-beam==2.19.0 fairness-indicators && pip list | grep -E \"analysis|beam|fairness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
    "from tensorflow_model_analysis.addons.fairness.view import widget_view\n",
    "from fairness_indicators.examples import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save((logdir/\"keras_saved_model\").as_posix(), save_format=\"tf\")\n",
    "print(f\"Saved in {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma_eval_result_path = logdir/'tfma_eval_result'\n",
    "\n",
    "slice_spec = [\n",
    "    tfma.slicer.SingleSliceSpec(), # Overall slice\n",
    "#     tfma.slicer.SingleSliceSpec(columns=[slice_selection]),\n",
    "]\n",
    "\n",
    "# Add the fairness metrics.\n",
    "add_metrics_callbacks = [\n",
    "  tfma.post_export_metrics.fairness_indicators(\n",
    "      thresholds=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "      labels_key=TARGET_COL\n",
    "      )\n",
    "]\n",
    "\n",
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=tfma_export_dir,\n",
    "    add_metrics_callbacks=add_metrics_callbacks)\n",
    "\n",
    "# Run the fairness evaluation.\n",
    "with beam.Pipeline() as pipeline:\n",
    "  _ = (\n",
    "      pipeline\n",
    "      | 'ReadData' >> beam.io.ReadFromTFRecord(validate_tf_file)\n",
    "      | 'ExtractEvaluateAndWriteResults' >>\n",
    "       tfma.ExtractEvaluateAndWriteResults(\n",
    "                 eval_shared_model=eval_shared_model,\n",
    "                 slice_spec=slice_spec,\n",
    "                 compute_confidence_intervals=compute_confidence_intervals,\n",
    "                 output_path=tfma_eval_result_path)\n",
    "  )\n",
    "\n",
    "eval_result = tfma.load_eval_result(output_path=tfma_eval_result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Save the model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save((logdir/\"keras_saved_model\").as_posix(), save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PredictionReport = namedtuple(\"PredictionReport\", \"probabilities predicted_rating confidence\")\n",
    "\n",
    "# Create a dataframe with all SELECTED_INP_COLS\n",
    "test_df = pd.DataFrame({\n",
    "    AGE: [\"45\"],\n",
    "    ZIP_CODE: [\"94086\"],\n",
    "    FAVE_SPORTS: [\"I do not like Sports\"]\n",
    "})\n",
    "\n",
    "probabilities = model.predict(transform_pd_X(test_df, SELECTED_INP_COLS))\n",
    "predicted_rating, confidence = np.argmax(probabilities), np.max(probabilities)\n",
    "\n",
    "PredictionReport(probabilities, predicted_rating, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Featurize using Feature Columns\n",
    "\n",
    "Create feature columns like one-hot, embeddings, bucketing from raw features created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_BATCH = next(iter(input_fn_train(3)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_feature_column(feature_column):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n",
    "    return feature_layer(EXAMPLE_BATCH).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age_fc = tf.feature_column.numeric_column(AGE, normalizer_fn=lambda x: (x - MEAN_AGE) / STD_AGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zip_fcs = [\n",
    "    tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            f\"{ZIP_CODE}{i}\", vocabulary_list=list(string.digits), \n",
    "            num_oov_buckets=1)\n",
    "    )\n",
    "    for i in range(FIRST_K_ZIP_DIGITS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_BATCH[AGE], test_feature_column(age_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "{k: v for k, v in EXAMPLE_BATCH.items() if k.startswith(ZIP_CODE)}, test_feature_column(zip_fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.concatenate(age_fc, zip_fcs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
