{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "\n",
    "The aim of the notebook is demo end to end pipeline for Ads prediction in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow, 2.1.0 on Python interpreter, sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any, Union, List\n",
    "from functools import partial\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from math import ceil\n",
    "from collections import namedtuple\n",
    "\n",
    "print(f\"Using Tensorflow, {tf.__version__} on Python interpreter, {sys.version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed, 1588211768\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = int(time.time())\n",
    "\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Using random seed, {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Dataset credits:\n",
    "```\n",
    "@inproceedings{roffo2016personality,\n",
    "  title={Personality in computational advertising: A benchmark},\n",
    "  author={Roffo, Giorgio and Vinciarelli, Alessandro},\n",
    "  booktitle={4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016},\n",
    "  pages={18},\n",
    "  year={2016}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../dataset/\")\n",
    "BATCH_SIZE = 4096 # bigger the batch, faster the training but bigger the RAM needed\n",
    "TARGET_COL = \"Rating\"\n",
    "\n",
    "# data files path are relative DATA_FOLDER\n",
    "users_ads_rating_csv = DATA_FOLDER/\"users-ads-without-gcp-ratings_OHE_MLB.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     43
    ]
   },
   "outputs": [],
   "source": [
    "USER_ID = \"UserId\"\n",
    "AD_ID = \"AdId\"\n",
    "AGE = \"Age\"\n",
    "ZIP_CODE = \"CapZipCode\"\n",
    "COUNTRIES_VISITED = \"Countriesvisited\"\n",
    "FAVE_SPORTS = \"FaveSports\"\n",
    "GENDER = \"Gender\"\n",
    "HOME_COUNTRY = \"Homecountry\"\n",
    "HOME_TOWN = \"Hometown\"\n",
    "INCOME = \"Income\"\n",
    "LAST_NAME = \"LastName\"\n",
    "MOST_LISTENED_MUSICS = \"Mostlistenedmusics\"\n",
    "MOST_READ_BOOKS = \"Mostreadbooks\"\n",
    "MOST_VISITED_WEBSITES = \"Mostvisitedwebsites\"\n",
    "MOST_WATCHED_MOVIES = \"Mostwatchedmovies\"\n",
    "MOST_WATCHED_TV_PROGRAMMES = \"Mostwatchedtvprogrammes\"\n",
    "NAME = \"Name\"\n",
    "PAYPAL = \"Paypal\"\n",
    "TIMEPASS = \"Timepass\"\n",
    "TYPE_OF_JOB = \"TypeofJob\"\n",
    "WEEKLY_WORKING_HOURS = \"Weeklyworkinghours\"\n",
    "FAVE1 = \"fave1\"\n",
    "FAVE10 = \"fave10\"\n",
    "FAVE2 = \"fave2\"\n",
    "FAVE3 = \"fave3\"\n",
    "FAVE4 = \"fave4\"\n",
    "FAVE5 = \"fave5\"\n",
    "FAVE6 = \"fave6\"\n",
    "FAVE7 = \"fave7\"\n",
    "FAVE8 = \"fave8\"\n",
    "FAVE9 = \"fave9\"\n",
    "UNFAVE1 = \"unfave1\"\n",
    "UNFAVE2 = \"unfave2\"\n",
    "UNFAVE3 = \"unfave3\"\n",
    "UNFAVE4 = \"unfave4\"\n",
    "UNFAVE5 = \"unfave5\"\n",
    "UNFAVE6 = \"unfave6\"\n",
    "ADFILEPATH = \"AdFilePath\"\n",
    "GENDER_F = \"Gender_F\"\n",
    "GENDER_M = \"Gender_M\"\n",
    "RATING = \"Rating\"\n",
    "\n",
    "# Read all columns as strings to avoid any errors\n",
    "COL_DEFAULTS = {\n",
    "    USER_ID: \"**\",\n",
    "    AD_ID: \"**\",\n",
    "    AGE: \"**\",\n",
    "    ZIP_CODE: \"**\",\n",
    "    COUNTRIES_VISITED: \"**\",\n",
    "    FAVE_SPORTS: \"**\",\n",
    "    GENDER: \"**\",\n",
    "    HOME_COUNTRY: \"**\",\n",
    "    HOME_TOWN: \"**\",\n",
    "    INCOME: \"**\",\n",
    "    LAST_NAME: \"**\",\n",
    "    MOST_LISTENED_MUSICS: \"**\",\n",
    "    MOST_READ_BOOKS: \"**\",\n",
    "    MOST_VISITED_WEBSITES: \"**\",\n",
    "    MOST_WATCHED_MOVIES: \"**\",\n",
    "    MOST_WATCHED_TV_PROGRAMMES: \"**\",\n",
    "    NAME: \"**\",\n",
    "    PAYPAL: \"**\",\n",
    "    TIMEPASS: \"**\",\n",
    "    TYPE_OF_JOB: \"**\",\n",
    "    WEEKLY_WORKING_HOURS: \"**\",\n",
    "    FAVE1: \"**\",\n",
    "    FAVE10: \"**\",\n",
    "    FAVE2: \"**\",\n",
    "    FAVE3: \"**\",\n",
    "    FAVE4: \"**\",\n",
    "    FAVE5: \"**\",\n",
    "    FAVE6: \"**\",\n",
    "    FAVE7: \"**\",\n",
    "    FAVE8: \"**\",\n",
    "    FAVE9: \"**\",\n",
    "    UNFAVE1: \"**\",\n",
    "    UNFAVE2: \"**\",\n",
    "    UNFAVE3: \"**\",\n",
    "    UNFAVE4: \"**\",\n",
    "    UNFAVE5: \"**\",\n",
    "    UNFAVE6: \"**\",\n",
    "    ADFILEPATH: \"**\",\n",
    "    GENDER_F: \"**\",\n",
    "    GENDER_M: \"**\",\n",
    "    RATING: \"**\"\n",
    "}\n",
    "\n",
    "# SELECTED_COLS = [AGE, ZIP_CODE, FAVE_SPORTS, GENDER, HOME_COUNTRY, HOME_TOWN, INCOME, MOST_LISTENED_MUSICS, MOST_READ_BOOKS, \n",
    "#                  MOST_VISITED_WEBSITES, MOST_WATCHED_MOVIES, MOST_WATCHED_TV_PROGRAMMES, TIMEPASS, TYPE_OF_JOB, WEEKLY_WORKING_HOURS, \n",
    "#                  FAVE1, FAVE2, FAVE3, FAVE4, FAVE5, FAVE6, FAVE7, FAVE8, FAVE9, FAVE10, UNFAVE1, UNFAVE2, UNFAVE3, UNFAVE4, UNFAVE5, \n",
    "#                  UNFAVE6, RATING]\n",
    "\n",
    "SELECTED_INP_COLS = [AGE, ZIP_CODE, FAVE_SPORTS, GENDER_F, GENDER_M]\n",
    "SELECTED_COLS = SELECTED_INP_COLS + [TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_dataset(batch_size=BATCH_SIZE, shuffle=True):\n",
    "    return tf.data.experimental.make_csv_dataset(\n",
    "        users_ads_rating_csv.as_posix(),\n",
    "        batch_size,\n",
    "        column_defaults={col:default for col, default in COL_DEFAULTS.items() if col in SELECTED_COLS},\n",
    "        select_columns=list(SELECTED_COLS),\n",
    "        label_name=None,\n",
    "        shuffle=shuffle,\n",
    "        shuffle_buffer_size=1000,\n",
    "        shuffle_seed=RANDOM_SEED,\n",
    "        sloppy=True,\n",
    "        ignore_errors=False # set true while training if required\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Age',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'24', b'26', b'22'], dtype=object)>),\n",
      "             ('CapZipCode',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'NW1 1EU', b'60638', b'54942'], dtype=object)>),\n",
      "             ('FaveSports',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
      "array([b'Individual sports\\xe2\\x80\\x8e (Tennis, Archery, ...)',\n",
      "       b'Nothing', b'Team sports (Footbal, Baseball, Rugby, ...) '],\n",
      "      dtype=object)>),\n",
      "             ('Rating',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'1.0', b'1.0', b'5.0'], dtype=object)>),\n",
      "             ('Gender_F',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'1', b'1', b'0'], dtype=object)>),\n",
      "             ('Gender_M',\n",
      "              <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'0', b'0', b'1'], dtype=object)>)])\n"
     ]
    }
   ],
   "source": [
    "for d in ad_dataset(3).take(1):\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_dataset_pd():\n",
    "    return pd.read_csv(users_ads_rating_csv, usecols=SELECTED_COLS, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CapZipCode</th>\n",
       "      <th>FaveSports</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Gender_F</th>\n",
       "      <th>Gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25582</th>\n",
       "      <td>37</td>\n",
       "      <td>SS2 5JL</td>\n",
       "      <td>Individual sportsâ€Ž (Tennis, Archery, ...)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27647</th>\n",
       "      <td>37</td>\n",
       "      <td>33024</td>\n",
       "      <td>Water sports</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21765</th>\n",
       "      <td>19</td>\n",
       "      <td>J8Y6C9</td>\n",
       "      <td>Team sports (Footbal, Baseball, Rugby, ...) , ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age CapZipCode                                         FaveSports  \\\n",
       "25582  37    SS2 5JL          Individual sportsâ€Ž (Tennis, Archery, ...)   \n",
       "27647  37      33024                                       Water sports   \n",
       "21765  19     J8Y6C9  Team sports (Footbal, Baseball, Rugby, ...) , ...   \n",
       "\n",
       "      Rating Gender_F Gender_M  \n",
       "25582    5.0        0        1  \n",
       "27647    4.0        1        0  \n",
       "21765    1.0        1        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset_pd().sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_project(d:Dict, cols:List[str]) -> Dict:\n",
    "    return {k:v for k, v in d.items() if k in cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexerForVocab:\n",
    "    def __init__(self, vocab_list:List[str], oov_index:int=0):\n",
    "        \"\"\"\n",
    "        Creates a string indexer for the vocabulary with out of vocabulary (oov) indexing\n",
    "        \"\"\"\n",
    "        self._vocab_map = {v:i+1 for i, v in enumerate(vocab_list)}\n",
    "        self._oov = oov_index\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Map for {len(self)} keys with 1 OOV key\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._vocab_map) + 1\n",
    "        \n",
    "    def index_of(self, item:str):\n",
    "        \"\"\"\n",
    "        Index of item in the vocabulary\n",
    "        \"\"\"\n",
    "        return self._vocab_map.get(item, self._oov)\n",
    "    \n",
    "    def index_of_mux(self, items:List[str]):\n",
    "        return [self.index_of(i) for i in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "Convert to a number and remove any outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Tensorflow Data Validation APIs data-exploration/tensorflow-data-validation.ipynb\n",
    "\n",
    "MEAN_AGE, STD_AGE, MEDIAN_AGE, MAX_AGE = 31.74, 12.07, 29, 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_age(age_str:tf.string, default_age=MEDIAN_AGE) -> int:\n",
    "    \"\"\"Typecast age to an integer and update outliers with the default\"\"\"\n",
    "    try:\n",
    "        age = int(age_str)\n",
    "        if age < 0 or age > MAX_AGE:\n",
    "            raise ValueError(f\"{age} is not a valid age\")\n",
    "    except:\n",
    "        age = default_age\n",
    "    normalized_age = (age - MEAN_AGE) / STD_AGE\n",
    "    return normalized_age\n",
    "\n",
    "def fix_age_tf(example:Dict, new_col_suffix=\"\"):\n",
    "    \"\"\"Wrap in a py_function for TF to run inside its execution graph\"\"\"\n",
    "#     example[AGE + new_col_suffix] = tf.py_function(fix_age, [example[AGE]], (tf.float32, ))\n",
    "    example[AGE + new_col_suffix] = tf.py_function(fix_age, [example[AGE]], tf.float32)\n",
    "    example[AGE + new_col_suffix] = tf.expand_dims(example[AGE + new_col_suffix], 0) # https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-579421744\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5128417564208783,\n",
       " -0.22700911350455663,\n",
       " -0.22700911350455663,\n",
       " -0.22700911350455663,\n",
       " -0.22700911350455663)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_age(\"50\"), fix_age(\"50.5\"), fix_age(\"-10\"), fix_age(\"bad_age_10\"), fix_age(\"300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'24'],\n",
      "       [b'26'],\n",
      "       [b'22']], dtype=object)>,\n",
      " 'Age_encoded': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[-0.6412593 ],\n",
      "       [-0.47555923],\n",
      "       [-0.8069594 ]], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'22'],\n",
      "       [b'62'],\n",
      "       [b'62']], dtype=object)>,\n",
      " 'Age_encoded': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[-0.8069594],\n",
      "       [ 2.5070422],\n",
      "       [ 2.5070422]], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'26'],\n",
      "       [b'26'],\n",
      "       [b'26']], dtype=object)>,\n",
      " 'Age_encoded': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[-0.47555923],\n",
      "       [-0.47555923],\n",
      "       [-0.47555923]], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'62'],\n",
      "       [b'22'],\n",
      "       [b'62']], dtype=object)>,\n",
      " 'Age_encoded': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[ 2.5070422],\n",
      "       [-0.8069594],\n",
      "       [ 2.5070422]], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'26'],\n",
      "       [b'22'],\n",
      "       [b'62']], dtype=object)>,\n",
      " 'Age_encoded': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
      "array([[-0.47555923],\n",
      "       [-0.8069594 ],\n",
      "       [ 2.5070422 ]], dtype=float32)>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fix_age_tf_fn = partial(fix_age_tf, new_col_suffix=\"_encoded\")\n",
    "for d in ad_dataset(1, True).map(fix_age_tf_fn).batch(3).take(5):\n",
    "    pprint(dict_project(d, [AGE, AGE + \"_encoded\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Code\n",
    "\n",
    "Prepare zip-code column for one-hot encoding each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ZIP_CODE, FIRST_K_ZIP_DIGITS = \"00000\", 2\n",
    "\n",
    "zip_code_indexer = IndexerForVocab(string.digits + string.ascii_lowercase + string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_zip_code_tensor(zip_code:tf.string, n_digits, indexer) -> List[str]:\n",
    "    \"\"\"Extracts the the first n_digits as a list\"\"\"\n",
    "    zip_digits = []\n",
    "    try:\n",
    "        if isinstance(zip_code, tf.Tensor):\n",
    "            zip_code = zip_code.numpy()[0].decode('ascii', errors=\"ignore\") # very ineffecient way\n",
    "        zip_digits = list(zip_code.strip()[:n_digits])\n",
    "    except:\n",
    "        zip_digits = list(DEFAULT_ZIP_CODE[:n_digits])\n",
    "    return tf.concat( [\n",
    "        tf.one_hot(\n",
    "            indexer.index_of(d), len(indexer)\n",
    "        ) for d in zip_digits\n",
    "    ], 0 )\n",
    "\n",
    "def fix_zip_code(zip_code:str, n_digits, indexer) -> List[str]:\n",
    "    \"\"\"Extracts the the first n_digits as a list\"\"\"\n",
    "    zip_digits = []\n",
    "    try:\n",
    "        zip_digits = list(zip_code.strip()[:n_digits])\n",
    "    except:\n",
    "        zip_digits = list(DEFAULT_ZIP_CODE[:n_digits])\n",
    "    return np.ravel(np.eye(len(indexer))[indexer.index_of_mux(zip_digits)])\n",
    "\n",
    "def fix_zip_code_tf(example:Dict, n_digits=FIRST_K_ZIP_DIGITS, indexer=zip_code_indexer, new_col_suffix=\"\"):\n",
    "    \"\"\"Creates new columns for the first n_digits in zip_code\"\"\"\n",
    "    fix_zip_code_fn = partial(fix_zip_code, n_digits=n_digits, indexer=indexer)\n",
    "    example[ZIP_CODE + new_col_suffix] = tf.py_function(fix_zip_code_fn, [example[ZIP_CODE]], tf.float32)\n",
    "    example[ZIP_CODE + new_col_suffix].set_shape(len(indexer) * n_digits)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_zip_code_indexer = IndexerForVocab(string.digits)\n",
    "\n",
    "(fix_zip_code(\"43556\", 10, test_zip_code_indexer),\n",
    "fix_zip_code(\"43556\", 2, test_zip_code_indexer),\n",
    "fix_zip_code(\"43556\", 4, test_zip_code_indexer),\n",
    "fix_zip_code(None, 3, test_zip_code_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_zip_code_indexer = IndexerForVocab(string.digits)\n",
    "\n",
    "(fix_zip_code(tf.constant([b\"43556\"], shape=(1,), dtype=tf.string), 10, test_zip_code_indexer),\n",
    "fix_zip_code(tf.constant([b\"43556\"], shape=(1,), dtype=tf.string), 2, test_zip_code_indexer),\n",
    "fix_zip_code(tf.constant([b\"43556\"], shape=(1,), dtype=tf.string), 4, test_zip_code_indexer),\n",
    "fix_zip_code(tf.constant([43556], shape=(1,), dtype=tf.int32), 4, test_zip_code_indexer),\\\n",
    "fix_zip_code(None, 3, test_zip_code_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CapZipCode': <tf.Tensor: shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'NW1 1EU'],\n",
      "       [b'60638'],\n",
      "       [b'54942'],\n",
      "       [b'54942'],\n",
      "       [b'15613']], dtype=object)>,\n",
      " 'CapZipCode_encoded': <tf.Tensor: shape=(5, 126), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)>}\n",
      "\n",
      "{'CapZipCode': <tf.Tensor: shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'15613'],\n",
      "       [b'60638'],\n",
      "       [b'60638'],\n",
      "       [b'60638'],\n",
      "       [b'15613']], dtype=object)>,\n",
      " 'CapZipCode_encoded': <tf.Tensor: shape=(5, 126), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)>}\n",
      "\n",
      "{'CapZipCode': <tf.Tensor: shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'54942'],\n",
      "       [b'15613'],\n",
      "       [b'60638'],\n",
      "       [b'54942'],\n",
      "       [b'15613']], dtype=object)>,\n",
      " 'CapZipCode_encoded': <tf.Tensor: shape=(5, 126), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fix_zip_code_tf_fn = partial(fix_zip_code_tf, new_col_suffix=\"_encoded\")\n",
    "for d in ad_dataset(1, True).map(fix_zip_code_tf_fn).batch(5).take(3):\n",
    "    pprint(dict_project(d, [ZIP_CODE, ZIP_CODE + \"_encoded\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Favorite Sports\n",
    "\n",
    "Two approaches,\n",
    "1. Consider the first `K` sports mentioned by each user and one-hot encode each separately\n",
    "2. Multi label binarize all the sports as there are only 15 unique sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAV_SPORTS_UNKNOWN = \"UNK_SPORT\"\n",
    "ALL_FAV_SPORTS = ['Olympic sports', 'Winter sports', 'Nothing', 'I do not like Sports', 'Equestrian sports', 'Skating sports', 'Precision sports', 'Hunting sports', 'Motor sports', 'Team sports', 'Individual sports', 'Other', 'Water sports', 'Indoor sports', 'Endurance sports']\n",
    "\n",
    "fav_sports_binarizer = MultiLabelBinarizer()\n",
    "fav_sports_binarizer.fit([ALL_FAV_SPORTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to write purely in TF graph\n",
    "# def fix_fav_sports(sports_str:str, topk=2, pad_constant=\"PAD_SPORT\") -> List[str]:\n",
    "#     sports = tf.strings.regex_replace(sports_str, r\"\\s*\\(.*,.*\\)\\s*\", \"\")\n",
    "#     sports = tf.strings.regex_replace(sports, r\"\\s*,\\s*\", \",\")\n",
    "#     sports = tf.strings.split(sports, \",\").numpy()[:topk]\n",
    "#     tf.print(sports.shape[0])\n",
    "#     right_pad_width = max(0, topk - sports.shape[0])\n",
    "#     result = np.pad(sports, (0, right_pad_width), constant_values=pad_constant) \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fav_sports_multi_select_str_to_list(sports_str:Union[str, tf.Tensor]) -> List[str]:\n",
    "    # remove commas that dont separate different user selections\n",
    "    # example, commas inside paranthesis of \"Individual sports (Tennis, Archery, ...)\" dont make new sports\n",
    "    if isinstance(sports_str, tf.Tensor):\n",
    "        sports_str = sports_str.numpy()[0].decode('ascii', errors=\"ignore\")\n",
    "    else:\n",
    "        sports_str = sports_str.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\") # remove non-ascii chars\n",
    "    sports = re.sub(r\"\\s*\\(.*,.*\\)\\s*\", \"\", sports_str)\n",
    "    return re.split(r\"\\s*,\\s*\", sports)\n",
    "\n",
    "def fix_fav_sports_mlb(sports_str:str) -> List[int]:\n",
    "    sports = fav_sports_multi_select_str_to_list(sports_str)\n",
    "    return fav_sports_binarizer.transform([sports])[0]\n",
    "\n",
    "def fix_fav_sports_firstk(sports_str:str, first_k:int, pad_constant:int) -> List[str]:\n",
    "    sports = fav_sports_multi_select_str_to_list(sports_str)\n",
    "    right_pad_width = first_k - len(sports_enc)\n",
    "    result = [sports + [pad_constant] * right_pad_width][:first_k]\n",
    "    return result\n",
    "\n",
    "def fix_fav_sports_tf(example:Dict, first_k=2, pad_constant=\"PAD_SPORT\", new_col_suffix:str=\"\"):\n",
    "    \"\"\"Existing column will not be overriden with new_col_suffix\"\"\"\n",
    "    example[FAVE_SPORTS + new_col_suffix] = tf.py_function(fix_fav_sports_mlb, [example[FAVE_SPORTS]], tf.float32)\n",
    "    example[FAVE_SPORTS + new_col_suffix].set_shape(len(ALL_FAV_SPORTS))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    fix_fav_sports_mlb(tf.constant([b\"Individual sports (Tennis, Archery, ...), Indoor sports, Endurance sports, Skating sports\"])),\n",
    "    fix_fav_sports_mlb(tf.constant([b\"Skating sports\"])),\n",
    "    fix_fav_sports_mlb(tf.constant([b\"Individual sports (Tennis, Archery, ...)\"])),\n",
    "    fix_fav_sports_mlb(tf.constant([b\"Indoor sports, Endurance sports, Skating sports\"])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    fix_fav_sports_mlb(\"Individual sports (Tennis, Archery, ...), Indoor sports, Endurance sports, Skating sports\"),\n",
    "    fix_fav_sports_mlb(\"Skating sports\"),\n",
    "    fix_fav_sports_mlb(\"Individual sports (Tennis, Archery, ...)\"),\n",
    "    fix_fav_sports_mlb(\"Indoor sports, Endurance sports, Skating sports\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FaveSports': <tf.Tensor: shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'Individual sports\\xe2\\x80\\x8e (Tennis, Archery, ...)'],\n",
      "       [b'Nothing'],\n",
      "       [b'Team sports (Footbal, Baseball, Rugby, ...) '],\n",
      "       [b'Team sports (Footbal, Baseball, Rugby, ...) '],\n",
      "       [b'I do not like Sports']], dtype=object)>,\n",
      " 'FaveSports_new': <tf.Tensor: shape=(5, 15), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)>}\n",
      "\n",
      "{'FaveSports': <tf.Tensor: shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'I do not like Sports'],\n",
      "       [b'Nothing'],\n",
      "       [b'Nothing'],\n",
      "       [b'Nothing'],\n",
      "       [b'I do not like Sports']], dtype=object)>,\n",
      " 'FaveSports_new': <tf.Tensor: shape=(5, 15), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fix_fav_sports_tf_fn = partial(fix_fav_sports_tf, new_col_suffix=\"_new\")\n",
    "for d in ad_dataset(1, True).map(fix_fav_sports_tf_fn).batch(5).take(2):\n",
    "    pprint(dict_project(d, [FAVE_SPORTS, FAVE_SPORTS + \"_new\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_CARDINALITY = 5 # not zero based indexing i.e. ratings range from 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(example:Dict):\n",
    "    y = tf.one_hot(\n",
    "        tf.cast(tf.strings.to_number(example[RATING], tf.float32), tf.int32), \n",
    "        RATINGS_CARDINALITY)\n",
    "    example.pop(RATING)\n",
    "    \n",
    "    return example, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_pd(rating_str:str):\n",
    "    return np.eye(RATINGS_CARDINALITY, dtype=int)[int(float(rating_str)) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tf() -> tf.data.Dataset:\n",
    "    return ad_dataset(1, True).\\\n",
    "        map(fix_age_tf, tf.data.experimental.AUTOTUNE).\\\n",
    "        map(fix_zip_code_tf, tf.data.experimental.AUTOTUNE).\\\n",
    "        map(fix_fav_sports_tf, tf.data.experimental.AUTOTUNE).\\\n",
    "        map(create_target, tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough?hl=en#create_a_tfdatadataset\n",
    "def pack_features_vector(features:Dict, labels, cols:List[str]=[AGE]):\n",
    "    \"\"\"Pack the features into a single array for the list of cols\"\"\"\n",
    "    # features = tf.stack(list(dict_project(features, cols).values()), axis=1)\n",
    "    features = tf.concat(list(dict_project(features, cols).values()), axis=1)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[-0.6412593 ],\n",
      "       [-0.47555923]], dtype=float32)>,\n",
      " <tf.Tensor: shape=(2, 1, 5), dtype=float32, numpy=\n",
      "array([[[0., 1., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 1., 0., 0., 0.]]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[-0.8069594],\n",
      "       [-0.8069594]], dtype=float32)>,\n",
      " <tf.Tensor: shape=(2, 1, 5), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 1., 0., 0., 0.]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for d in create_dataset_tf().batch(2).map(pack_features_vector).take(2):\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note down the number of columns from here to provide in keras input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pd_X(df:pd.DataFrame, inp_cols:List[str]):\n",
    "    \"\"\"Original dataframe will be modified\"\"\"\n",
    "    df[AGE] = df[AGE].apply(lambda age: [fix_age(age)])\n",
    "    df[ZIP_CODE] = df[ZIP_CODE].apply(lambda zc: fix_zip_code(zc, n_digits=2, indexer=zip_code_indexer))\n",
    "    df[FAVE_SPORTS] = df[FAVE_SPORTS].apply(fix_fav_sports_mlb)\n",
    "    df[GENDER_F] = df[GENDER_F].apply(lambda gender_f: [int(gender_f)])\n",
    "    df[GENDER_M] = df[GENDER_M].apply(lambda gender_m: [int(gender_m)])\n",
    "    df[\"X\"] = df[inp_cols].apply(np.concatenate, axis=1)\n",
    "    # TODO: vectorize, else inefficient to sequentially loop over all example\n",
    "    X = np.array([x for x in df[\"X\"]])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pd_y(df:pd.DataFrame, target_col:str):\n",
    "    \"\"\"Original dataframe will be modified\"\"\"\n",
    "    df[\"y\"] = df[target_col].apply(create_target_pd)\n",
    "    # TODO: vectorize, else inefficient to sequentially loop over all example\n",
    "    y = np.array([y for y in df[\"y\"]])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_pd(inp_cols:List[str]=SELECTED_INP_COLS, target_col:str=TARGET_COL, fraction:float=1) -> pd.DataFrame:\n",
    "    \"\"\"Prepare the dataset for training on a fraction of all input data\"\"\"\n",
    "    df = ad_dataset_pd().sample(frac=fraction)\n",
    "    return transform_pd_X(df, inp_cols), transform_pd_y(df, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input builders\n",
    "def input_fn_train(batch_size=10):\n",
    "    return create_dataset().\\\n",
    "        shuffle(2 * batch_size).batch(batch_size, drop_remainder=True).\\\n",
    "        map(pack_features_vector, tf.data.experimental.AUTOTUNE).\\\n",
    "        cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def input_fn_eval(batch_size=10, cache=True):\n",
    "    # TODO: use dataset's skip & take to create train and validation datasets\n",
    "    val_dataset = create_dataset(test_files).batch(batch_size)\n",
    "    if cache: val_dataset = val_dataset.cache()\n",
    "    return val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def input_fn_predict():\n",
    "    # return tf.data.Dataset.from_tensor_slices({\"x\": tf.cast(X_test, tf.int32)}).batch(1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visual Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[-0.47555923],\n",
      "       [-0.6412593 ]], dtype=float32)>,\n",
      " <tf.Tensor: shape=(2, 1, 5), dtype=float32, numpy=\n",
      "array([[[0., 1., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 1., 0., 0., 0.]]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[2.5070422],\n",
      "       [2.5070422]], dtype=float32)>,\n",
      " <tf.Tensor: shape=(2, 1, 5), dtype=float32, numpy=\n",
      "array([[[0., 1., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 1., 0., 0., 0.]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for d in input_fn_train(2).take(2):\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "Monitor training and other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9057f595c23a30b7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9057f595c23a30b7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs (started 11:19:27 ago; pid 77286)\n"
     ]
    }
   ],
   "source": [
    "notebook.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Create a model and train using high level APIs like `tf.keras` and `tf.estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 920 ms, total: 17.1 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train_dataset = input_fn_train(BATCH_SIZE)\n",
    "X, y = create_dataset_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE THE EPOCHS VALUE\n",
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 1000\n",
    "keras_model_metrics = [\"accuracy\"]\n",
    "train_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging tensorboard data at logs/20200430-021723\n"
     ]
    }
   ],
   "source": [
    "logdir = Path(\"logs\")/datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    logdir, \n",
    "    histogram_freq=max(1, ceil(EPOCHS / 20)), # to control the amount of logging\n",
    "#     embeddings_freq=epochs,\n",
    ")\n",
    "print(f\"Logging tensorboard data at {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 20)                2900      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 3,005\n",
      "Trainable params: 3,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_shape=(X.shape[1],), activation=tf.keras.layers.LeakyReLU()),\n",
    "    tf.keras.layers.Dense(RATINGS_CARDINALITY , activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(\n",
    "        learning_rate=0.003,\n",
    "        clipvalue=0.5\n",
    "    ), \n",
    "#     optimizer=tf.keras.optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True), \n",
    "#     optimizer=tf.keras.optimizers.RMSprop(lr),\n",
    "#     loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=keras_model_metrics\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, accuracy:0.3800,  loss:1.5123,  val_accuracy:0.5437,  val_loss:1.4133,  \n",
      "....................................................................................................\n",
      "Epoch: 100, accuracy:0.6156,  loss:1.0363,  val_accuracy:0.6110,  val_loss:1.0493,  \n",
      "....................................................................................................\n",
      "Epoch: 200, accuracy:0.6161,  loss:1.0190,  val_accuracy:0.6136,  val_loss:1.0379,  \n",
      "....................................................................................................\n",
      "Epoch: 300, accuracy:0.6170,  loss:1.0124,  val_accuracy:0.6151,  val_loss:1.0313,  \n",
      "....................................................................................................\n",
      "Epoch: 400, accuracy:0.6149,  loss:1.0087,  val_accuracy:0.6128,  val_loss:1.0287,  \n",
      "....................................................................................................\n",
      "Epoch: 500, accuracy:0.6165,  loss:1.0083,  val_accuracy:0.6126,  val_loss:1.0289,  \n",
      "....................................................................................................\n",
      "Epoch: 600, accuracy:0.6165,  loss:1.0069,  val_accuracy:0.6126,  val_loss:1.0270,  \n",
      "....................................................................................................\n",
      "Epoch: 700, accuracy:0.6162,  loss:1.0055,  val_accuracy:0.6143,  val_loss:1.0273,  \n",
      "....................................................................................................\n",
      "Epoch: 800, accuracy:0.6156,  loss:1.0056,  val_accuracy:0.6125,  val_loss:1.0279,  \n",
      "....................................................................................................\n",
      "Epoch: 900, accuracy:0.6165,  loss:1.0050,  val_accuracy:0.6139,  val_loss:1.0270,  \n",
      "....................................................................................................CPU times: user 1min 56s, sys: 41 s, total: 2min 37s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_histories.append(model.fit(\n",
    "    X, y,\n",
    "    BATCH_SIZE,\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[tensorboard_callback, tfdocs.modeling.EpochDots()],\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.004807571834988\n",
      "accuracy 0.6155556\n",
      "val_loss 1.0253095727496677\n",
      "val_accuracy 0.61430556\n"
     ]
    }
   ],
   "source": [
    "histories_dict = train_histories[-1].history\n",
    "for metric in histories_dict.keys():\n",
    "    print(metric, histories_dict[metric][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Save the model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/20200428-102840/keras_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save((logdir/\"keras_saved_model\").as_posix(), save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionReport(probabilities=array([[0.9737386 , 0.0085956 , 0.01389753, 0.00218609, 0.00158213]],\n",
       "      dtype=float32), predicted_rating=0, confidence=0.9737386)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictionReport = namedtuple(\"PredictionReport\", \"probabilities predicted_rating confidence\")\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    AGE: [\"45\"],\n",
    "    ZIP_CODE: [\"94086\"],\n",
    "    FAVE_SPORTS: [\"I do not like Sports\"]\n",
    "})\n",
    "\n",
    "probabilities = model.predict(transform_pd_X(test_df, SELECTED_INP_COLS))\n",
    "predicted_rating, confidence = np.argmax(probabilities), np.max(probabilities)\n",
    "\n",
    "PredictionReport(probabilities, predicted_rating, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Featurize using Feature Columns\n",
    "\n",
    "Create feature columns like one-hot, embeddings, bucketing from raw features created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EXAMPLE_BATCH = next(iter(input_fn_train(3)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 142), dtype=float32, numpy=\n",
       "array([[-0.8069594 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.5070422 ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.47555923,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_feature_column(feature_column):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n",
    "    return feature_layer(EXAMPLE_BATCH).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age_fc = tf.feature_column.numeric_column(AGE, normalizer_fn=lambda x: (x - MEAN_AGE) / STD_AGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zip_fcs = [\n",
    "    tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            f\"{ZIP_CODE}{i}\", vocabulary_list=list(string.digits), \n",
    "            num_oov_buckets=1)\n",
    "    )\n",
    "    for i in range(FIRST_K_ZIP_DIGITS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'Age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-31bfc493571b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEXAMPLE_BATCH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAGE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage_fc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/env37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'Age'"
     ]
    }
   ],
   "source": [
    "EXAMPLE_BATCH[AGE], test_feature_column(age_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "{k: v for k, v in EXAMPLE_BATCH.items() if k.startswith(ZIP_CODE)}, test_feature_column(zip_fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.concatenate(age_fc, zip_fcs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
